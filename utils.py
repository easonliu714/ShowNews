import os
import json
import random
import re
import aiohttp
import asyncio
from datetime import datetime
from pathlib import Path
from bs4 import BeautifulSoup
from telegram import Bot
from telegram.constants import ParseMode

PLATFORMS = [
    "KKTIX", "æ‹“å…ƒå”®ç¥¨", "OPENTIX", "å¯¬å®", "å¹´ä»£å”®ç¥¨", "UDNå”®ç¥¨ç¶²", "iBonå”®ç¥¨"
]

DETAIL_URL_WHITELIST = {
    "KKTIX": re.compile(r"^https?://[a-z0-9-]+\.kktix\.cc/events/[A-Za-z0-9-_]+", re.I),
    "æ‹“å…ƒå”®ç¥¨": re.compile(r"^https?://(www\.)?tixcraft\.com/activity/detail/[A-Za-z0-9_-]+", re.I),
    "OPENTIX": re.compile(r"^https?://(www\.)?opentix\.life/event/\d+", re.I),
    "å¯¬å®": re.compile(r"^https?://(www\.)?kham\.com\.tw/application/UTK02/UTK0201_\.aspx\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "å¹´ä»£å”®ç¥¨": re.compile(r"^https?://(www\.)?ticket\.com\.tw/application/UTK02/UTK0201_\.aspx\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "UDNå”®ç¥¨ç¶²": re.compile(r"^https?://(www\.)?tickets\.udnfunlife\.com/application/UTK02/UTK0201_\.aspx\?PRODUCT_ID=[A-Z0-9]+", re.I),
    "iBonå”®ç¥¨": re.compile(r"^https?://(www\.)?ticket\.ibon\.com\.tw/", re.I),
}

TOKEN = os.getenv("TG_BOT_TOKEN")
CHAT_ID = os.getenv("TG_CHAT_ID")
bot = Bot(token=TOKEN) if TOKEN and CHAT_ID else None

RUN_LOG = "run.log"
LOG_FILE = 'Show_News_log.json'

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/124.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 13_5_1) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.5 Safari/605.1.15',
]
REQUEST_HEADERS = {
    'User-Agent': random.choice(USER_AGENTS),
    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
}

def escape_markdown_v2(text: str) -> str:
    if not isinstance(text, str): text = str(text)
    escape_chars = r'_*[]()~`>#+=|{}.!-'
    return re.sub(f'([{re.escape(escape_chars)}])', r'\\\1', text)

def append_log_run(msg):
    log_path = Path(RUN_LOG)
    mode = 'a' if log_path.exists() else 'w'
    with log_path.open(mode, encoding='utf-8') as f:
        f.write(f"[{datetime.now()}] {msg}\n")
    print(f"[DEBUG] {msg}")

def load_json_file(file_path):
    if not os.path.exists(file_path):
        return {}
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            return json.load(f)
    except Exception:
        return {}

def save_json_file(data, file_path):
    with open(file_path, 'w', encoding='utf-8') as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def load_log():
    return load_json_file(LOG_FILE)

def save_log(log):
    save_json_file(log, LOG_FILE)

def safe_get_text(element, default="è©³å…§æ–‡"):
    return element.get_text(strip=True) if element and hasattr(element, 'get_text') and element.get_text(strip=True) else default

def get_event_category_from_title(title):
    if not title: return "å…¶ä»–"
    title_lower = title.lower()
    mapping = {
        "éŸ³æ¨‚æœƒ/æ¼”å”±æœƒ": ["éŸ³æ¨‚æœƒ", "æ¼”å”±æœƒ", "live", "concert", "éŸ³æ¨‚ç¯€"],
        "éŸ³æ¨‚åŠ‡/æ­ŒåŠ‡": ["éŸ³æ¨‚åŠ‡", "æ­ŒåŠ‡", "musical", "opera"],
        "æˆ²åŠ‡è¡¨æ¼”": ["æˆ²åŠ‡", "èˆå°åŠ‡", "å–œåŠ‡", "åŠ‡å ´", "åŠ‡åœ˜"],
        "èˆè¹ˆè¡¨æ¼”": ["èˆè¹ˆ", "èŠ­è•¾"],
        "å±•è¦½/åšè¦½": ["å±•è¦½", "ç‰¹å±•", "è—è¡“å±•", "ç¾è¡“é¤¨"],
        "è¦ªå­æ´»å‹•": ["è¦ªå­", "å…’ç«¥", "å¯¶å¯¶", "å®¶åº­"],
        "é›»å½±æ”¾æ˜ ": ["é›»å½±", "å½±å±•"],
        "é«”è‚²è³½äº‹": ["è³½äº‹", "é¦¬æ‹‰æ¾", "è·¯è·‘", "çƒè³½"],
        "è¬›åº§/å·¥ä½œåŠ": ["è¬›åº§", "å·¥ä½œåŠ"],
        "å¨›æ¨‚è¡¨æ¼”": ["ç¶œè—", "è„«å£ç§€", "ç§€å ´"],
        "å…¶ä»–": ["æ—…éŠ", "ç¾é£Ÿ", "å…¬ç›Š"]
    }
    for cat, keys in mapping.items():
        if any(k in title_lower for k in keys): return cat
    return "å…¶ä»–"

async def fetch_platform_events_list(session, platform):
    url_selector_map = {
        "KKTIX": ("https://kktix.com/events", 'a[href*="/events/"]', "KKTIX"),
        "OPENTIX": ("https://www.opentix.life/event", 'a[href*="/event/"]', "OPENTIX"),
        "æ‹“å…ƒå”®ç¥¨": ("https://tixcraft.com/activity", 'a[href*="/activity/detail/"]', "æ‹“å…ƒå”®ç¥¨"),
        "å¯¬å®": ("https://kham.com.tw/", 'a[href*="/application/UTK02/UTK0201_"]', "å¯¬å®"),
        "å¹´ä»£å”®ç¥¨": ("https://ticket.com.tw/", 'a[href*="/application/UTK02/UTK0201_"]', "å¹´ä»£å”®ç¥¨"),
        "UDNå”®ç¥¨ç¶²": ("https://tickets.udnfunlife.com/", 'a[href*="/application/UTK02/UTK0201_"]', "UDNå”®ç¥¨ç¶²"),
        "iBonå”®ç¥¨": ("https://ticket.ibon.com.tw/", 'a[href^="https://ticket.ibon.com.tw/"]', "iBonå”®ç¥¨")
    }
    if platform not in url_selector_map:
        return []
    start_url, selector, plat_name = url_selector_map[platform]
    try:
        async with session.get(start_url, headers=REQUEST_HEADERS, timeout=20) as resp:
            html = await resp.text()
        soup = BeautifulSoup(html, "html.parser")
        links = soup.select(selector)
        events = []
        wl = DETAIL_URL_WHITELIST.get(platform)
        seen_urls = set()
        for link in links:
            href = link.get('href','')
            title = safe_get_text(link)
            if not href or not title or len(title)<3: 
                continue
            if not href.startswith("http"):
                if platform=="OPENTIX":
                    href = "https://www.opentix.life" + href
                elif platform=="KKTIX":
                    href = "https://kktix.com" + href
                elif platform=="æ‹“å…ƒå”®ç¥¨":
                    href = "https://tixcraft.com" + href
                elif platform=="å¯¬å®":
                    href = "https://kham.com.tw" + href
                elif platform=="å¹´ä»£å”®ç¥¨":
                    href = "https://ticket.com.tw" + href
                elif platform=="UDNå”®ç¥¨ç¶²":
                    href = "https://tickets.udnfunlife.com" + href
            if wl and not wl.match(href):
                continue
            if href in seen_urls:
                continue
            events.append({
                "title": title.strip(), "url": href, "platform": plat_name, "type": get_event_category_from_title(title)
            })
            seen_urls.add(href)
        return events
    except Exception as e:
        append_log_run(f"{platform} åˆ—è¡¨æ“·å–éŒ¯èª¤: {e}")
        return []

async def extract_event_details_simple(url, platform, session, list_event=None):
    details = {'date': 'è©³å…§æ–‡', 'location': 'è©³å…§æ–‡', 'ticket_date': 'è©³å…§æ–‡', 'description': '', 'title': 'è©³å…§æ–‡'}
    if list_event and 'title' in list_event:
        details['title'] = list_event['title']
    try:
        async with session.get(url, headers=REQUEST_HEADERS, timeout=20, ssl=False) as resp:
            html = await resp.text()
        soup = BeautifulSoup(html, "html.parser")
        title_candidates = [soup.find("title"), soup.find("h1"),
                            soup.find("meta", attrs={"property": "og:title"}),
                            soup.find("meta", attrs={"name": "twitter:title"})]
        for c in title_candidates:
            if c:
                new_title = c.get('content','').strip() if c.name=='meta' else safe_get_text(c)
                if new_title and len(new_title)>5:
                    details['title']=new_title
                    break
        if not details['description']:
            desc_candidates = [
                soup.find("meta", {"property":"og:description"}),
                soup.find("meta", {"name": "description"})
            ]
            for elem in desc_candidates:
                if elem:
                    desc_text = elem.get('content','').strip()
                    if len(desc_text)>150: desc_text=desc_text[:150]+"..."
                    details['description'] = desc_text
                    break
        page_text = soup.get_text(" ", strip=True)
        date_match = re.search(r'(\d{4}[./]\d{1,2}[./]\d{1,2})', page_text)
        if date_match: details['date'] = date_match.group(1).strip()
        loc_match = None

        if platform == "å¹´ä»£å”®ç¥¨":
            # ç§»é™¤å¤šé¤˜å­—ä¸²ï¼Œå¹´ä»£è¡¨æ¼”å ´åœ°å¯èƒ½é ˆç”¨CSSæŠ“å–ï¼Œæš«ç”¨regexæ›¿ä»£ï¼Œæ‚¨å¯é€²ä¸€æ­¥å¾®èª¿
            details['title'] = details['title'].replace("å¹´ä»£å”®ç¥¨ |", "").strip()
            # å–å¾—è¼ƒç²¾æº–åœ°é» (ç¤ºç¯„ç”¨ regexæ‹‰å– "æ¼”å‡ºåœ°é»" æ¬„ä½é™„è¿‘æ–‡å­—)
            loc_match = re.search(r'æ¼”å‡ºåœ°é»[:ï¼š]\s*([^\sï¼Œã€‚]+)', page_text)

        elif platform == "UDNå”®ç¥¨ç¶²":
            details['title'] = details['title'].replace(" | udnå”®ç¥¨ç¶²", "").strip()
            loc_match = re.search(r'åœ°é»[:ï¼š]\s*([^\sï¼Œã€‚]+)', page_text)

        elif platform == "å¯¬å®":
            # å› å¯¬å®æ¨™é¡Œå…¨æ˜¯ç³»çµ±åï¼Œæ”¹å¾åˆ—è¡¨titleè£œè¶³ï¼Œè©³ç´°é å®šä½ç¤ºæ„:
            if list_event and list_event.get('title'):
                details['title'] = list_event['title']
            loc_match = re.search(r'å ´åœ°[:ï¼š]\s*([^\sï¼Œã€‚]+)', page_text)

        else:
            loc_match = re.search(r'(?:åœ°é»|å ´åœ°|æ¼”å‡ºåœ°é»)[:ï¼š]?\s*([^\sï¼Œã€‚]+)', page_text)

        if loc_match:
            details['location'] = loc_match.group(1).strip()

        return details
    except Exception as e:
        append_log_run(f"è©³ç´°é é¢æå–å¤±æ•—ï¼š{url} - {e}")
        return details

async def send_telegram_message_with_retry(event, is_init=False, downgraded=False, max_retries=3):
    if not bot: 
        return False, "Bot not ready"
    for attempt in range(max_retries):
        try:
            header = "ğŸ†• æ–°å¢æ´»å‹•é€šçŸ¥"
            title = escape_markdown_v2(event.get('title', 'è©³å…§æ–‡'))
            event_type = escape_markdown_v2(event.get('type', 'è©³å…§æ–‡'))
            event_date = escape_markdown_v2(event.get('date', 'è©³å…§æ–‡'))
            event_location = escape_markdown_v2(event.get('location', 'è©³å…§æ–‡'))
            event_platform = escape_markdown_v2(event.get('platform', 'è©³å…§æ–‡'))
            lines = [
                header,
                f"ğŸ« {title}",
                f"ğŸ“ é¡å‹ï¼š{event_type}",
                f"ğŸ“… æ—¥æœŸï¼š{event_date}",
                f"ğŸ—º åœ°é»ï¼š{event_location}",
                f"ğŸ§¾ å¹³å°ï¼š{event_platform}"
            ]
            if downgraded:
                lines.append("âš ï¸ è©³é è³‡è¨Šå—é™ï¼Œè«‹é»æ“Šé€£çµæŸ¥çœ‹")
            lines.append(f"\nğŸ“Œ [é»æˆ‘æŸ¥çœ‹è©³æƒ…]({event.get('url','')})")
            msg = "\n".join(lines)
            if len(msg)>4096:
                msg = "\n".join(lines[:6]) + f"\n\nğŸ“Œ [é»æˆ‘æŸ¥çœ‹è©³æƒ…]({event.get('url','')})"

            await bot.send_message(chat_id=CHAT_ID, text=msg, parse_mode=ParseMode.MARKDOWN_V2)
            # å»¶é²2ç§’é¿å… flood control
            await asyncio.sleep(2)
            return True, None
        except Exception as err:
            append_log_run(f"ç™¼é€å¤±æ•— (ç¬¬{attempt+1}æ¬¡)ï¼š{event.get('title','(ç„¡æ¨™é¡Œ)')} => {err}")
            wait = 5 * (attempt + 1)
            await asyncio.sleep(wait)
    return False, "Send failed"

async def send_platform_summary_message(platform_stats):
    if not bot:
        return
    try:
        lines = ["ğŸ“Š æ´»å‹•æ¨æ’­çµ±è¨ˆå ±å‘Š"]
        for plat, stat in platform_stats.items():
            count_new = stat.get('new',0)
            count_sent = stat.get('sent',0)
            lines.append(f"ğŸ§¾ {plat}: æ–°æ´»å‹•æ•¸ {count_new}ï¼ŒæˆåŠŸæ¨é€ {count_sent}")
        lines.append(f"æ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        msg = "\n".join(lines)
        await bot.send_message(chat_id=CHAT_ID, text=msg)
    except Exception as e:
        append_log_run(f"æ¨æ’­çµ±è¨ˆè¨Šæ¯å¤±æ•—: {e}")

async def test_crawl_and_notify():
    async with aiohttp.ClientSession() as session:
        platform_stats = {}
        log = load_log()

        for platform in PLATFORMS:
            events = await fetch_platform_events_list(session, platform)
            platform_stats[platform] = {'new':0, 'sent':0}
            if not events:
                append_log_run(f"{platform} æœªæŠ“å–åˆ°æ–°æ´»å‹•")
                continue
            # å»é‡èˆ‡éå»ç´€éŒ„æ¯”å°
            seen_url = set()
            filtered = []
            for e in events:
                if e['url'] not in seen_url:
                    seen_url.add(e['url'])
                    filtered.append(e)
            new_events = [e for e in filtered if e['url'] not in log]

            platform_stats[platform]['new'] = len(new_events)
            count_sent = 0
            for event in new_events[:5]:  # æ¯å¹³å°æœ€å¤šåªç™¼5å‰‡
                details = await extract_event_details_simple(event['url'], event['platform'], session, list_event=event)
                merged = event.copy()
                merged.update(details)
                ok, err = await send_telegram_message_with_retry(merged, is_init=True)
                if ok:
                    count_sent += 1
                    log[merged['url']] = {'title': merged.get('title', event['title'])}
                    save_log(log)
            platform_stats[platform]['sent'] = count_sent

        # æœ€å¾Œç™¼é€å¹³å°å½™æ•´è¨Šæ¯
        await send_platform_summary_message(platform_stats)

        return {
            "success": True,
            "platform_stats": platform_stats
        }
